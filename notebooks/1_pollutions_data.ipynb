{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Source\" data-toc-modified-id=\"Data-Source-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Source</a></span></li><li><span><a href=\"#Imports-and-Update-The-Latest-Data\" data-toc-modified-id=\"Imports-and-Update-The-Latest-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports and Update The Latest Data</a></span></li><li><span><a href=\"#Historical-air-4-Thai-Data\" data-toc-modified-id=\"Historical-air-4-Thai-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Historical air 4 Thai Data</a></span></li><li><span><a href=\"#Power-Plants\" data-toc-modified-id=\"Power-Plants-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Power Plants</a></span></li><li><span><a href=\"#Weather-Files\" data-toc-modified-id=\"Weather-Files-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Weather Files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assemble-from-raw-data\" data-toc-modified-id=\"Assemble-from-raw-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Assemble from raw data</a></span></li><li><span><a href=\"#Fill-the-missing-value-in-Weather-Data\" data-toc-modified-id=\"Fill-the-missing-value-in-Weather-Data-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Fill the missing value in Weather Data</a></span></li><li><span><a href=\"#Update-weather-all-cities\" data-toc-modified-id=\"Update-weather-all-cities-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Update weather all cities</a></span></li></ul></li><li><span><a href=\"#Holiday-In-Thailand\" data-toc-modified-id=\"Holiday-In-Thailand-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Holiday In Thailand</a></span></li><li><span><a href=\"#Hotspots-Data\" data-toc-modified-id=\"Hotspots-Data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Hotspots Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source \n",
    "1. Berekely Earth 'http://berkeleyearth.lbl.gov/air-quality/maps/cities/Thailand/'\n",
    "2. Screaped Air4Thai Data 'http://air4thai.pcd.go.th/webV2/history/'\n",
    "3. CDC Data 'https://www.cmuccdc.org/download_json/'\n",
    "4. Old Air4Thai data from Thailand EPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Update The Latest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#  Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "from pathlib import Path\n",
    "\n",
    "#  always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.imports import *\n",
    "from src.data.download_data import *\n",
    "from src.data.read_data import *\n",
    "from src.data.fire_data import *\n",
    "from src.data.weather_data import *\n",
    "from src.gen_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_folder='../data/pm25/'\n",
    "a4th_folder='../data/air4thai_hourly/'\n",
    "cdc_folder = '../data/cdc_data/'\n",
    "aqm_folder = '../data/aqm_hourly2/'\n",
    "w_folder = '../data/weather_cities/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 805931 / 805931"
     ]
    }
   ],
   "source": [
    "# Data from Berkeley Earth Projects: \n",
    "download_b_data(data_folder='../data/pm25/', url='http://berkeleyearth.lbl.gov/air-quality/maps/cities/Thailand/')\n",
    "get_city_info(data_folder='../data/pm25/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                             | 0/404 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stations 407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 404/404 [17:08<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "download_cdc_data(station_url='https://www.cmuccdc.org/api/ccdc/stations', \n",
    "                  dl_url= 'https://www.cmuccdc.org/download_json/', \n",
    "                  data_folder='../data/cdc_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02t', '03t', '05t', '08t', '10t', '11t', '12t', '13t', '14t', '16t', '17t', '18t', '19t', '20t', '21t', '22t', '24t', '25t', '26t', '27t', '28t', '29t', '30t', '31t', '32t', '33t', '34t', '35t', '36t', '37t', '38t', '39t', '40t', '41t', '42t', '43t', '44t', '46t', '47t', '50t', '52t', '53t', '54t', '57t', '58t', '59t', '60t', '61t', '62t', '63t', '67t', '68t', '69t', '70t', '71t', '72t', '73t', '74t', '75t', '76t', '77t', '79t', '80t', '81t', '82t', '83t', '84t', 'm1', 'm4', 'm8', 'm9', 'o10', 'o20', 'o22', 'o23', 'o24', 'o25', 'o26', 'o27', 'o28', 'o29']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:24, 24.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:50, 25.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [01:16, 25.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [01:42, 25.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [02:08, 25.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [02:34, 25.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [03:00, 25.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [03:26, 26.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [03:53, 26.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [04:19, 26.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [04:45, 26.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "12it [05:12, 26.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "13it [05:38, 26.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "14it [06:04, 26.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "15it [06:31, 26.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "16it [06:57, 26.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "17it [07:24, 26.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "18it [07:50, 26.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "19it [08:16, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "20it [08:43, 26.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "21it [09:09, 26.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "22it [09:35, 26.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "23it [10:02, 26.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "24it [10:24, 25.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "25it [10:50, 25.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "26it [11:16, 25.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "27it [11:42, 25.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "28it [12:09, 26.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "29it [12:35, 26.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "30it [13:02, 26.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "31it [13:29, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "32it [13:55, 26.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "33it [14:21, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "34it [14:47, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "35it [15:14, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "36it [15:40, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "37it [16:07, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "38it [16:33, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "39it [16:59, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "40it [17:26, 26.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "41it [17:52, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "42it [18:18, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "43it [18:45, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "44it [19:11, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "45it [19:37, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "46it [20:04, 26.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "47it [20:30, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "48it [20:56, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "49it [21:23, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "50it [21:49, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "51it [22:15, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "52it [22:42, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "53it [23:08, 26.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "54it [23:35, 26.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "55it [24:01, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "56it [24:27, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "57it [24:54, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "58it [25:20, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "59it [25:46, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "60it [26:13, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "61it [26:39, 26.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "62it [27:05, 26.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "63it [27:31, 26.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "64it [27:58, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "65it [28:24, 26.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "66it [28:51, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "67it [29:17, 26.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "68it [29:44, 26.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "69it [30:10, 26.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "70it [30:36, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "71it [31:03, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "72it [31:29, 26.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "73it [31:55, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "74it [32:22, 26.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "75it [32:48, 26.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "76it [33:15, 26.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "77it [33:41, 26.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "78it [34:08, 26.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "79it [34:34, 26.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "80it [34:54, 24.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "81it [35:19, 26.17s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "update_last_air4Thai(url='http://air4thai.pcd.go.th/webV2/history/',data_folder='../data/air4thai_hourly/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical air 4 Thai Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "124\n",
      "125\n",
      "['35t', '36t', 'm9']\n"
     ]
    }
   ],
   "source": [
    "# load stations information\n",
    "station_info_file = aqm_folder + 'stations_locations.json'\n",
    "with open(station_info_file, 'r',encoding=\"utf8\") as f:\n",
    "    station_info = json.load(f)\n",
    "station_info = station_info['stations']\n",
    "\n",
    "# find stations in Chiangmai and parase that files\n",
    "cm_station_ids = []\n",
    "for i, stations in enumerate(station_info):\n",
    "    if 'Chiang Mai' in stations['areaEN']:\n",
    "        # ignore station that start with o\n",
    "        if 'o' not in stations['stationID']:\n",
    "            cm_station_ids.append(stations['stationID'])\n",
    "            print(i)\n",
    "print(cm_station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save filename ../data/aqm_hourly2/process/35t.csv\n",
      "save filename ../data/aqm_hourly2/process/36t.csv\n"
     ]
    }
   ],
   "source": [
    "# parase historical data \n",
    "for station_id in cm_station_ids:\n",
    "    # find all files that start with this stations\n",
    "    p = Path(aqm_folder)\n",
    "    filenames = []\n",
    "    for i in p.glob('**/*.xlsx'):\n",
    "        if station_id in i.name:\n",
    "            filenames.append(str(i))\n",
    "        \n",
    "    # if filename exist load that file\n",
    "    if len(filenames) >0:\n",
    "\n",
    "        save_filename = aqm_folder + 'process/' + station_id + '.csv'\n",
    "        print('save filename', save_filename)\n",
    "        station_data = read_his_xl(filenames[0])\n",
    "        #print(station_data.head())\n",
    "\n",
    "        # save the data if the dataframe is not empty\n",
    "        if len(station_data)> 0:\n",
    "            station_data.to_csv(save_filename,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data1 = pd.read_csv(aqm_folder + 'process/35t.csv')\n",
    "cm_data1['datetime'] = pd.to_datetime(cm_data1['datetime'])\n",
    "cm_data1 = cm_data1.set_index('datetime')\n",
    "# keep only gas columns\n",
    "cm_data1 = cm_data1[['CO', 'O3', 'NO2', 'SO2', 'PM10', 'PM2.5']]\n",
    "\n",
    "cm_data2 = pd.read_csv(aqm_folder + 'process/36t.csv')\n",
    "cm_data2['datetime'] = pd.to_datetime(cm_data2['datetime'])\n",
    "cm_data2 = cm_data2.set_index('datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Power Plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%82%E0%B8%A3%E0%B8%87%E0%B9%84%E0%B8%9F%E0%B8%9F%E0%B9%89%E0%B8%B2%E0%B9%83%E0%B8%99%E0%B9%84%E0%B8%97%E0%B8%A2'\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_power_stations_in_Thailand'\n",
    "table_list = pd.read_html(url)\n",
    "len(table_list)\n",
    "p_folder = '../data/power_plants/'\n",
    "for i, table in enumerate(table_list):\n",
    "    table.to_csv(p_folder + f'table_eng{i}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = glob('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/weather_cities/*/')\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bkk 13\n",
      "chiang-mai 36\n",
      "chiang-rai 25\n",
      "kungming 24\n",
      "luang-prabang 24\n",
      "sikhottabong 39\n",
      "tada_u 24\n",
      "tak 26\n",
      "yangong 29\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    city_name = Path(folder).name\n",
    "    parent_folder = Path(folder).parent\n",
    "    w_files = glob(folder + '/*.csv')\n",
    "    print(city_name, len(w_files))\n",
    "    filename = str(Path(folders[0]).parent) + '/' + city_name + '.csv'\n",
    "    \n",
    "    # concatenate all files \n",
    "    df_all = pd.DataFrame()\n",
    "    for file in w_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "    \n",
    "    # drop missing value \n",
    "    df_all['datetime'] = pd.to_datetime(df_all['date'])\n",
    "    df_all.drop('date',axis=1, inplace=True)\n",
    "    df_all = df_all.sort_values('datetime')\n",
    "    df_all = df_all.drop_duplicates('datetime', ignore_index=True)\n",
    "    \n",
    "    # save file\n",
    "    df_all.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing date for chiang-mai data \n",
    "df_all = pd.read_csv('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/weather_cities/chiang-mai.csv')\n",
    "df_all['datetime'] = pd.to_datetime(df_all['datetime'] )\n",
    "# find exisiting date \n",
    "ex_date = df_all['datetime'].dt.strftime('%Y-%m-%d').unique()\n",
    "ex_date = set(ex_date)\n",
    "# calculate the datelist \n",
    "start_date = datetime(2000, 10, 1)\n",
    "stop_date = datetime.now()\n",
    "date_range = pd.date_range(start_date, stop_date).strftime('%Y-%m-%d')\n",
    "missing_date = list(set(date_range).difference(ex_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city_name': 'Mueang Chiang Mai', 'province': 'Chiang Mai', 'country': 'Thailand', 'station_name': 'Chiang Mai International Airport Station', 'specific_url': 'th/mueang-chiang-mai/', 'latitude': '18.8 °N', 'longitude': '98.97 °E'}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/weather_cities/weather_station_info.json','r') as f:\n",
    "    station_dict_list = json.load(f)\n",
    "   \n",
    "i = 0 \n",
    "city_json = station_dict_list[i]\n",
    "print(city_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Benny/Documents/Fern/aqi_thailand2/data/weather_cities/chiang-mai/mueang-chiang-mai_weather.csv 2000-10-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\src\\data\\dl_weather.py:20: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 20 of the file ..\\src\\data\\dl_weather.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(innerhtml)\n"
     ]
    }
   ],
   "source": [
    " bad_date_df = scrape_weather(city_json, date_range=missing_date, data_folder='C:/Users/Benny/Documents/Fern/aqi_thailand2/data/weather_cities/chiang-mai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333126, 11)\n"
     ]
    }
   ],
   "source": [
    "# weather data \n",
    "filename = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/data/weather_cities/chiang-mai.csv'\n",
    "wea = pd.read_csv(filename)\n",
    "wea['datetime']  = pd.to_datetime(wea['datetime'])\n",
    "# roud datetiem to whole 30 mins \n",
    "wea['datetime'] = wea['datetime'].dt.round('30T')\n",
    "\n",
    "dates = wea['datetime'].dropna().dt.date.unique()\n",
    "\n",
    "# fill in the missing value\n",
    "new_datetime = pd.date_range(start=dates[0], end=dates[-1], freq='30T') \n",
    "new_weather = pd.DataFrame(new_datetime, columns=['datetime'])\n",
    "new_weather = new_weather.merge(wea, on='datetime',how='left')\n",
    "print(new_weather.shape)\n",
    "\n",
    "# remove strange T reading\n",
    "lowest_t = 5 \n",
    "idx = new_weather[new_weather['Temperature(C)']< lowest_t].index\n",
    "new_weather.loc[idx,['Temperature(C)','Dew Point(C)','Humidity(%)']] = np.nan\n",
    "\n",
    "highest_t = 60\n",
    "idx = new_weather[new_weather['Temperature(C)']> highest_t].index\n",
    "new_weather.loc[idx,['Temperature(C)','Dew Point(C)','Humidity(%)']] = np.nan\n",
    "\n",
    "new_weather = new_weather.fillna(method='ffill',limit=12)\n",
    "new_weather = new_weather.fillna(method='bfill',limit=12)\n",
    "new_weather = new_weather.set_index('datetime')\n",
    "new_weather = new_weather.dropna(how='all').reset_index()\n",
    "new_weather.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble from raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather folder\n",
    "w_folder = '../data/weather_cities/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract station information\n",
    "city_names = ['Mueang Chiang Mai', 'Mueang Chiang Rai', 'Mueang Tak','Bangkok','Yangon', 'Tada-U', 'Sikhottabong', 'Luang Prabang District','Kunming']\n",
    "weather_station_info = find_weather_stations(city_names, weather_json_file=w_folder+'weather_station_info.json')\n",
    "len(weather_station_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/weather_cities/Mueang_Chiang_Mai.csv\n",
      "36\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Mueang_Chiang_Rai.csv\n",
      "25\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Mueang_Tak.csv\n",
      "26\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Bangkok.csv\n",
      "13\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime', 'Temperature(F)', 'Dew Point(F)',\n",
      "       'Precip Accum(in)'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Yangon.csv\n",
      "29\n",
      "Index(['header_url', 'datetime', 'Time', 'Temperature(C)', 'Dew Point(C)',\n",
      "       'Humidity(%)', 'Wind', 'Wind Speed(mph)', 'Wind Gust(mph)',\n",
      "       'Pressure(in)', 'Precip.(in)', 'Condition'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Tada-U.csv\n",
      "24\n",
      "Index(['header_url', 'datetime', 'Time', 'Temperature(C)', 'Dew Point(C)',\n",
      "       'Humidity(%)', 'Wind', 'Wind Speed(mph)', 'Wind Gust(mph)',\n",
      "       'Pressure(in)', 'Precip.(in)', 'Condition'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Sikhottabong.csv\n",
      "39\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Luang_Prabang_District.csv\n",
      "24\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n",
      "../data/weather_cities/Kunming.csv\n",
      "24\n",
      "Index(['Time', 'Temperature(C)', 'Dew Point(C)', 'Humidity(%)', 'Wind',\n",
      "       'Wind Speed(mph)', 'Wind Gust(mph)', 'Pressure(in)', 'Precip.(in)',\n",
      "       'Condition', 'datetime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# concatenate raw data files\n",
    "for city_json in weather_station_info: \n",
    "    # read existing file \n",
    "    city_name = ('_').join(city_json['city_name'].split(' '))\n",
    "    current_filename = w_folder + city_name + '.csv'\n",
    "    print(current_filename)\n",
    "    \n",
    "    # locate file in the city folder\n",
    "    files = glob(w_folder + city_name + '/*.csv')\n",
    "    print(len(files))\n",
    "    \n",
    "    weather_all = pd.DataFrame()\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            if 'date'== df.columns[-1]:\n",
    "                df.columns = df.columns.str.replace('date','datetime')\n",
    "            weather_all = pd.concat([weather_all, df],ignore_index=True)\n",
    "        \n",
    "    print(weather_all.columns)\n",
    "    weather_all.to_csv(current_filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the unit of windspeed \n",
    "files = glob(w_folder + '/*.csv')\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df[['Wind Speed(mph)', 'Wind Gust(mph)']] = df[['Wind Speed(mph)', 'Wind Gust(mph)']]*1.60934\n",
    "    df[['Wind Speed(mph)', 'Wind Gust(mph)']] = df[['Wind Speed(mph)', 'Wind Gust(mph)']].round(0)\n",
    "    df.columns = df.columns.str.replace('mph', 'kmph')\n",
    "    df.to_csv(file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate datetime \n",
    "files = glob(w_folder + '/*.csv')\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime')\n",
    "    df = df.drop_duplicates('datetime')\n",
    "    df.to_csv(file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing value in raw Weather Data \n",
    "for file in files:\n",
    "    df = pd.read_csv(files[3])\n",
    "    df = fill_missing_weather(df,limit=12)\n",
    "    df.to_csv(file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weather all cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/weather_cities/Mueang_Chiang_Mai.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime.now()\n",
    "city_json = weather_station_info[i]\n",
    "# read existing file \n",
    "city_name = ('_').join(city_json['city_name'].split(' '))\n",
    "current_filename = w_folder + city_name + '.csv'\n",
    "print(current_filename)\n",
    "\n",
    "# obtain a list of existed dates\n",
    "wea = pd.read_csv(current_file)\n",
    "wea['datetime'] = pd.to_datetime(wea['datetime'])\n",
    "# find exisiting date \n",
    "ex_date = wea['datetime'].dt.strftime('%Y-%m-%d').unique()\n",
    "ex_date = set(ex_date)\n",
    "\n",
    "# calculate the missing dates \n",
    "date_range = pd.date_range(start_date, end_date).strftime('%Y-%m-%d')\n",
    "missing_date = list(set(date_range).difference(ex_date))\n",
    "missing_date.sort()\n",
    "len(missing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updateing file: ../data/weather_cities/Mueang_Chiang_Mai.csv\n",
      "missing date 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:28, 28.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:58, 29.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [01:29, 29.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [01:59, 29.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [02:30, 29.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [02:59, 29.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [03:29, 29.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [03:58, 29.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [04:27, 29.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [04:57, 29.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [05:26, 29.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "12it [05:55, 29.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "13it [06:27, 30.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "14it [06:56, 29.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "15it [07:25, 29.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "16it [07:55, 29.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "17it [08:25, 29.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "18it [08:54, 29.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "19it [09:24, 29.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "20it [09:52, 29.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "21it [10:21, 29.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "22it [10:51, 29.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "23it [11:22, 29.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "24it [11:51, 29.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "25it [12:19, 29.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "26it [12:48, 29.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "27it [13:19, 29.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "28it [13:48, 29.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "29it [14:19, 29.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "30it [14:48, 29.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "31it [15:17, 29.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "32it [15:46, 29.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "33it [16:14, 29.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "34it [16:43, 28.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "35it [17:12, 28.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "36it [17:41, 28.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "37it [18:09, 28.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "38it [18:37, 28.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "39it [19:07, 29.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "40it [19:37, 29.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "41it [20:06, 29.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "42it [20:47, 32.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "43it [21:16, 31.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "44it [21:45, 30.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "45it [22:13, 30.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "46it [22:42, 29.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "47it [23:11, 29.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "48it [23:39, 29.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "49it [24:07, 28.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "50it [24:36, 28.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "51it [25:04, 28.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "52it [25:33, 28.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "53it [26:02, 28.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "54it [26:32, 29.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "55it [27:01, 29.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "56it [27:30, 29.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "57it [27:59, 29.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "58it [28:32, 30.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "59it [29:03, 30.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "60it [29:32, 30.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "61it [30:02, 30.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "62it [30:31, 29.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "63it [30:59, 29.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "64it [31:29, 29.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "65it [31:58, 29.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "66it [32:28, 29.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "67it [32:57, 29.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "68it [33:27, 29.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "69it [33:55, 29.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "70it [34:24, 28.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "71it [34:52, 28.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "72it [35:21, 28.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "73it [35:49, 28.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "74it [36:19, 28.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "75it [37:00, 32.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "76it [37:29, 31.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "77it [37:58, 30.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "78it [38:26, 29.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "79it [38:56, 29.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "80it [39:25, 29.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "81it [39:54, 29.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "82it [40:24, 29.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "83it [40:52, 29.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "84it [41:25, 30.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "85it [41:54, 29.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "86it [42:22, 29.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "87it [42:51, 29.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "88it [43:19, 28.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "89it [43:48, 28.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "90it [44:16, 28.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "91it [44:44, 28.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "92it [45:13, 28.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "93it [45:42, 28.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "94it [46:11, 28.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "95it [46:40, 28.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "96it [47:09, 28.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "97it [47:38, 29.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "98it [48:07, 29.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "99it [48:36, 28.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "100it [49:05, 29.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "101it [49:33, 28.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "102it [50:01, 28.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "103it [50:30, 28.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "104it [51:02, 29.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "105it [51:34, 30.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "106it [52:05, 30.60s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for city_json in weather_station_info:\n",
    "    \n",
    "    start_date = datetime(2000,10,1)\n",
    "    end_date = datetime.now()\n",
    "    update_weather(city_json, data_folder=w_folder, start_date=start_date, end_date=end_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holiday In Thailand\n",
    "\n",
    "https://www.timeanddate.com/holidays/thailand/2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2000,2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape holiday from all websites\n",
    "th_holiday = pd.DataFrame()\n",
    "for year in years:\n",
    "    url = f'https://www.timeanddate.com/holidays/thailand/{year}'\n",
    "    df = pd.read_html(url)[0]\n",
    "    df['year'] = year\n",
    "    th_holiday = pd.concat([th_holiday,df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_holiday.columns = ['Date', 'day_of_week','name','type','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_holiday = th_holiday[~th_holiday['Date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_holiday['date'] = th_holiday['Date'] + ', ' + th_holiday['year'].astype(str)\n",
    "th_holiday['date'] = pd.to_datetime(th_holiday['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_holiday.to_csv('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/th_holiday.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotspots Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stationID': '36t', 'nameTH': 'โรงเรียนยุพราชวิทยาลัย ', 'nameEN': 'Yupparaj Wittayalai School', 'areaTH': 'ต.ศรีภูมิ อ.เมือง, เชียงใหม่', 'areaEN': 'Si Phum, Meuang, Chiang Mai', 'stationType': 'GROUND', 'lat': '18.7909205', 'long': '98.9881062', 'LastUpdate': {'date': '2020-03-27', 'time': '03:00', 'PM25': {'value': '87', 'unit': 'µg/m³'}, 'PM10': {'value': '110', 'unit': 'µg/m³'}, 'O3': {'value': 'N/A', 'unit': 'ppb'}, 'CO': {'value': '1.09', 'unit': 'ppm'}, 'NO2': {'value': '-', 'unit': 'ppb'}, 'SO2': {'value': '2', 'unit': 'ppb'}, 'AQI': {'Level': '4', 'aqi': '192'}}}\n"
     ]
    }
   ],
   "source": [
    "# load stations information for Chiangmai\n",
    "station_info_file = aqm_folder + 'stations_locations.json'\n",
    "with open(station_info_file, 'r',encoding=\"utf8\") as f:\n",
    "    station_info = json.load(f)\n",
    "station_info = station_info['stations']\n",
    "\n",
    "print(station_info[124])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2117.0 11019.0\n"
     ]
    }
   ],
   "source": [
    "# obtain the lat and long in km\n",
    "lat_km = (merc_y(station_info[124]['lat'])/1E3 ).round()\n",
    "long_km = (merc_x(station_info[124]['long'])/1E3).round()\n",
    "print(lat_km,  long_km )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file map folder \n",
    "m_files = glob('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/fire_map/world_2000-2020/M6/*.csv')\n",
    "v_files = glob('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/fire_map/world_2000-2020/V1/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keep the file spot with distance 1000 km from the station\n",
    "distance = 1000 # km\n",
    "\n",
    "filename = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/data/cm_proc/file_m.csv'\n",
    "\n",
    "\n",
    "file_all = pd.DataFrame()\n",
    "\n",
    "for file in tqdm(m_files):\n",
    "    f = pd.read_csv(file)\n",
    "     \n",
    "    # convert lat \n",
    "    f['lat_km'] = (f['latitude'].apply(merc_y)/1E3).round().astype(int)\n",
    "    f['long_km'] = (merc_x(f['longitude'])/1E3).round().astype(int)\n",
    "    # remove by lat \n",
    "    f = f[f['lat_km'] <= (lat_km+1000)]\n",
    "    f = f[f['lat_km'] >= (lat_km-1000)]\n",
    "    # remove by long \n",
    "\n",
    "    f = f[f['long_km'] <= (long_km+1000)]\n",
    "    f = f[f['long_km'] >= (long_km-1000)]\n",
    "     \n",
    "    file_all = pd.concat([file_all,f],ignore_index=True)\n",
    "        \n",
    "        \n",
    "file_all.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benny\\pyenv\\geo\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3901918, 17)\n"
     ]
    }
   ],
   "source": [
    "m_filename = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/data/cm_proc/file_m.csv'\n",
    "process_fire_data(m_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the file spot with distance 1000 km from the station\n",
    "distance = 1000 # km\n",
    "\n",
    "filename = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/data/cm_proc/file_v.csv'\n",
    "\n",
    "fire_all = pd.DataFrame()\n",
    "\n",
    "for file in v_files:\n",
    "    f = pd.read_csv(file)\n",
    "     \n",
    "    # convert lat \n",
    "    f['lat_km'] = (f['latitude'].apply(merc_y)/1E3).round()\n",
    "    f['long_km'] = (merc_x(f['longitude'])/1E3).round()\n",
    "    # remove by lat \n",
    "    f = f[f['lat_km'] <= (lat_km+1000)]\n",
    "    f = f[f['lat_km'] >= (lat_km-1000)]\n",
    "    # remove by long \n",
    "\n",
    "    f = f[f['long_km'] <= (long_km+1000)]\n",
    "    f = f[f['long_km'] >= (long_km-1000)]\n",
    "    \n",
    "    fire_all = pd.concat([fire_all,f],ignore_index=True)  \n",
    "     \n",
    "        \n",
    "fire_all.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benny\\pyenv\\geo\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (10,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9092222, 17)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Benny/Documents/Fern/aqi_thailand2/data/cm_proc/file_v.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
